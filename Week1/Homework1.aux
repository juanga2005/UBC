\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Fundamentals}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Matrix Notation}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Linear Function I}{1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Linear Function II}{2}}
\newlabel{eqnpartI}{{1}{2}}
\newlabel{eqnpartII}{{2}{2}}
\newlabel{eqnpartIII}{{3}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Quadratic Function}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.4}L2-regularized weighted least squares}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.5}Weighted L2-regularized probit regression}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Cross-Validation}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Plot leastSquaresRBFL2.m}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Linear regression with Gaussian Radial Basis Functions.\relax }}{6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{figRBFregression}{{1}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Computational Cost}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}Cross-Validation to set $\lambda $ and $\sigma $}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Error plot for the $10-$fold cross validation procedure using the average error in the two norm between the test set and the prediction by getting $w$ from the training set.\relax }}{8}}
\newlabel{figCV}{{2}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Fitting by using the values given by the cross validation procedure.\relax }}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}MAP estimation}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Laplace distribution}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Gaussians}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}Poisson Likelihood}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Convex Functions}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Minimizing Strictly-Convex Quadratic Functions}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Projection}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Least Square}{12}}
\newlabel{eqnnicenorm}{{4}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Weighted Least Squares Shruking to $w^{(0)}$}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Proving Convexity}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Negative Log}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Quadratic with positive semi-definite A}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Any norm}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Logistic Regression}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5}regularized regression with arbitrary p-norm}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.6}Support Vector Regression}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.7}3 largest-magnitude elements}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Robust Regression}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}robustRegression.m}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparison for the robust regression with SVM and least squares.\relax }}{17}}
\newlabel{figregressions}{{4}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}SVM as a Linear Program}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}SVM Regression}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Numerical Optimization}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradient Descent and Newton's Method}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Replacing $\alpha \leftarrow \alpha /2$}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Reseting $alpha$ to $\alpha \leftarrow -\alpha \frac  {v^{T}\nabla f(w)}{v^{T}v}$}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Fixing the steps size to $\alpha =\frac  {1}{L}$ with $L=\frac  {1}{4}\qopname  \relax m{max}\{eig(X^{T}X)\}+\lambda $}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Changing the direction to $d=(\nabla ^{2} f(w))^{-1}\nabla {f})$}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Hessian-Free Newton}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Multi-Class Logistic Regression}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Classification result minimizing the loss function associated to the softmax probabilities.\relax }}{21}}
